---
title: HYPER-V 上的系統資源成本 |Microsoft 文件
ms.custom: ''
ms.date: 06/08/2017
ms.prod: biztalk-server
ms.reviewer: ''
ms.suite: ''
ms.tgt_pltfrm: ''
ms.topic: article
ms.assetid: 9f25a76c-1c41-41c0-b28d-d7473dbe1cd1
caps.latest.revision: 8
author: MandiOhlinger
ms.author: mandia
manager: anneta
ms.openlocfilehash: 491c71a446829ddddfc4d7c55053b94dcf7fc9d1
ms.sourcegitcommit: 8418b1a8f38b7f56979cd6e203f0b591e2f40fe1
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 03/28/2018
ms.locfileid: "26009423"
---
# <a name="system-resource-costs-on-hyper-v"></a><span data-ttu-id="1cfdf-102">HYPER-V 上的系統資源成本</span><span class="sxs-lookup"><span data-stu-id="1cfdf-102">System Resource Costs on Hyper-V</span></span>
## <a name="system-resource-costs-associated-with-running-a-guest-operating-system-on-hyper-v"></a><span data-ttu-id="1cfdf-103">HYPER-V 上執行客體作業系統與相關聯的系統資源成本</span><span class="sxs-lookup"><span data-stu-id="1cfdf-103">System Resource Costs Associated with Running a Guest Operating System on Hyper-V</span></span>  
 <span data-ttu-id="1cfdf-104">伺服器虛擬化軟體，沒有與執行以支援在 HYPER-V 上執行的客體作業系統的虛擬化程式碼相關聯的負擔量。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-104">As with any server virtualization software, there is a certain amount of overhead associated with running the virtualization code required to support guest operating systems running on Hyper-V.</span></span> <span data-ttu-id="1cfdf-105">下列清單摘要說明當 HYPER-V 虛擬機器上執行的客體作業系統相關聯的特定資源的額外負荷：</span><span class="sxs-lookup"><span data-stu-id="1cfdf-105">The following list summarizes the overhead associated with specific resources when running guest operating systems on Hyper-V virtual machines:</span></span>  
  
### <a name="cpu-overhead"></a><span data-ttu-id="1cfdf-106">CPU 額外負荷</span><span class="sxs-lookup"><span data-stu-id="1cfdf-106">CPU Overhead</span></span>  
 <span data-ttu-id="1cfdf-107">CPU 負擔執行客體作業系統的 HYPER-V 虛擬機器中找不到 9 與 12%之間的範圍。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-107">The CPU overhead associated with running a guest operating system in a Hyper-V virtual machine was found to range between 9 and 12%.</span></span>  <span data-ttu-id="1cfdf-108">例如，通常在 HYPER-V 虛擬機器上執行客體作業系統必須可用 88 91%可用來在實體硬體上執行相同作業系統的 CPU 資源。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-108">For example, a guest operating system running on a Hyper-V virtual machine typically had available 88-91% of the CPU resources available to an equivalent operating system running on physical hardware.</span></span>  
  
### <a name="memory-overhead"></a><span data-ttu-id="1cfdf-109">記憶體額外負荷</span><span class="sxs-lookup"><span data-stu-id="1cfdf-109">Memory Overhead</span></span>  
 <span data-ttu-id="1cfdf-110">針對 HYPER-V 主機電腦，與 HYPER-V 虛擬機器上執行客體作業系統相關聯的記憶體成本發現是大約 300 MB 的 hypervisor，加上 32 MB 的第一個 GB 的 RAM 配置給每個虛擬機器，加上另一個 8 MB針對每個其他 GB 的 RAM 配置給每個虛擬機器。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-110">For the Hyper-V host computer, the memory cost associated with running a guest operating system on a Hyper-V virtual machine was observed to be approximately 300 MB for the hypervisor, plus 32 MB for the first GB of RAM allocated to each virtual machine, plus another 8 MB for every additional GB of RAM allocated to each virtual machine.</span></span> <span data-ttu-id="1cfdf-111">如需為 HYPER-V 虛擬機器上執行的客體作業系統的記憶體配置方式的詳細資訊，請參閱中的 < 最佳化記憶體效能 > 一節[檢查清單： 在 HYPER-V 上的 最佳化效能](~/technical-guides/checklist-optimizing-performance-on-hyper-v.md)。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-111">For more information about allocating memory to guest operating systems running on a Hyper-V virtual machine, see the “Optimizing Memory Performance” section in [Checklist: Optimizing Performance on Hyper-V](~/technical-guides/checklist-optimizing-performance-on-hyper-v.md).</span></span>  
  
### <a name="network-overhead"></a><span data-ttu-id="1cfdf-112">網路額外負荷</span><span class="sxs-lookup"><span data-stu-id="1cfdf-112">Network Overhead</span></span>  
 <span data-ttu-id="1cfdf-113">直接能歸因於執行客體作業系統的 HYPER-V 虛擬機器中觀察到的網路延遲是少於 1 毫秒和客體作業系統通常維護網路輸出佇列的長度小於 1。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-113">Network latency directly attributable to running a guest operating system in a Hyper-V virtual machine was observed to be less than 1 ms and the guest operating system typically maintained a network output queue length of less than one.</span></span> <span data-ttu-id="1cfdf-114">如需有關測量網路輸出佇列長度的詳細資訊，請參閱中的 < 測量網路效能 > 一節[檢查清單： 在 HYPER-V 上的測量效能](../technical-guides/checklist-measuring-performance-on-hyper-v.md)。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-114">For more information about measuring the network output queue length, see the “Measuring Network Performance” section in [Checklist: Measuring Performance on Hyper-V](../technical-guides/checklist-measuring-performance-on-hyper-v.md).</span></span>  
  
### <a name="disk-overhead"></a><span data-ttu-id="1cfdf-115">磁碟的額外負荷</span><span class="sxs-lookup"><span data-stu-id="1cfdf-115">Disk Overhead</span></span>  
 <span data-ttu-id="1cfdf-116">使用傳遞磁碟功能時在 HYPER-V 中，找不到磁碟與 HYPER-V 虛擬機器中執行來賓作業系統相關聯的 I/O 額外負荷介於 6 和 8%的範圍。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-116">When using the passthrough disk feature in Hyper-V, disk I/O overhead associated with running a guest operating system in a Hyper-V virtual machine was found to range between 6 and 8 %.</span></span> <span data-ttu-id="1cfdf-117">比方說，通常在 HYPER-V 上執行客體作業系統有可用的 92 94%的磁碟 I/O 可測量的效能評定工具 IOMeter 開放原始碼磁碟效能的實體硬體上執行對等作業系統。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-117">For example, a guest operating system running on Hyper-V typically had available 92-94% of the disk I/O available to an equivalent operating system running on physical hardware as measured by the open source disk performance benchmarking tool IOMeter.</span></span>  
  
 <span data-ttu-id="1cfdf-118">測量在 HYPER-V 主機或客體作業系統上使用 「 效能監視器的磁碟延遲的相關資訊，請參閱中的 < 測量磁碟 I/O 效能 > 一節[檢查清單： 在 HYPER-V 上的測量效能](../technical-guides/checklist-measuring-performance-on-hyper-v.md)。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-118">For information about measuring disk latency on a Hyper-V host or guest operating system using Performance Monitor, see the “Measuring Disk I/O Performance” section in [Checklist: Measuring Performance on Hyper-V](../technical-guides/checklist-measuring-performance-on-hyper-v.md).</span></span>  
  
 <span data-ttu-id="1cfdf-119">本節的其餘部分提供 BizTalk Server 上磁碟效能的背景資訊，描述的測試組態參數使用，並提供取得的測試結果的摘要。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-119">The remainder of this section provides background information on BizTalk Server disk performance, describes the test configuration parameters used, and provides a summary of test results obtained.</span></span>  
  
#### <a name="disk-performance-when-running-a-biztalk-server-solution-on-hyper-v"></a><span data-ttu-id="1cfdf-120">HYPER-V 上執行 BizTalk Server 解決方案時，磁碟效能</span><span class="sxs-lookup"><span data-stu-id="1cfdf-120">Disk Performance When Running a BizTalk Server Solution on Hyper-V</span></span>  
 <span data-ttu-id="1cfdf-121">BizTalk Server 是極資料庫密集的應用程式可能需要建立的 SQL Server 中的多達 13 個資料庫。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-121">BizTalk Server is an extremely database intensive application that may require the creation of up to 13 databases in SQL Server.</span></span> <span data-ttu-id="1cfdf-122">BizTalk Server 保存到磁碟的資料很棒的頻率，此外，這樣做，MSDTC 交易的內容中。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-122">BizTalk Server persists data to disk with great frequency and furthermore, does so within the context of an MSDTC transaction.</span></span> <span data-ttu-id="1cfdf-123">因此，資料庫效能極為重要的任何 BizTalk Server 解決方案的整體效能。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-123">Therefore, database performance is paramount to the overall performance of any BizTalk Server solution.</span></span> <span data-ttu-id="1cfdf-124">HYPER-V 提供綜合 SCSI 控制器，而這兩者使用模擬的 IDE 裝置，例如透過提供效能優勢明顯 IDE 篩選器驅動程式隨附 Virtual Server 2005。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-124">Hyper-V provides a synthetic SCSI controller and an IDE filter driver which both provide significant performance benefits over using an emulated IDE device such as is provided with Virtual Server 2005.</span></span>  
  
 <span data-ttu-id="1cfdf-125">設定為使用 SCSI 控制站的資料磁碟區的磁碟。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-125">Configure disks for data volumes using the SCSI controller.</span></span> <span data-ttu-id="1cfdf-126">這樣會保證會安裝整合服務，因為如果而不需要安裝 HYPER-V 整合服務是使用模擬的 IDE 控制器，已安裝 HYPER-V 整合服務，可以只安裝 SCSI 控制器。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-126">This will guarantee that the integration services are installed because the SCSI controller can only be installed if Hyper-V integration services are installed whereas the emulated IDE controller is available without installing Hyper-V integration services.</span></span> <span data-ttu-id="1cfdf-127">使用 SCSI 控制站或 integration services 提供 IDE 篩選器驅動程式執行磁碟 I/O 是明顯優於磁碟 I/O 效能所提供的模擬的 IDE 控制器。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-127">Disk I/O performed using either the SCSI controller or the IDE filter driver provided with integration services is significantly better than disk I/O performance provided with the emulated IDE controller.</span></span> <span data-ttu-id="1cfdf-128">因此，若要確保最佳的磁碟 I/O 效能中 HYPER-V 虛擬化環境中的資料檔案，在主機和客體作業系統上安裝 integration services 及設定資料磁碟區的磁碟與綜合 SCSI 控制器。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-128">Therefore, to ensure optimal disk I/O performance for the data files in a Hyper-V virtualized environment, install integration services on both the host and guest operating system and configure disks for data volumes with the synthetic SCSI controller.</span></span> <span data-ttu-id="1cfdf-129">對於跨越多個資料磁碟機的極大量的儲存體 I/O 工作負載，每個 VHD 應該附加到另一個更佳的整體效能的綜合 SCSI 控制器。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-129">For highly intensive storage I/O workloads that span multiple data drives, each VHD should be attached to a separate synthetic SCSI controller for better overall performance.</span></span> <span data-ttu-id="1cfdf-130">此外，每個 VHD 應該儲存在個別的實體磁碟或 Lun 上。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-130">In addition, each VHD should be stored on separate physical disks or LUNs.</span></span>  
  
#### <a name="measuring-passthrough-disk-performance"></a><span data-ttu-id="1cfdf-131">測量穿通磁碟效能</span><span class="sxs-lookup"><span data-stu-id="1cfdf-131">Measuring PassThrough Disk Performance</span></span>  
 <span data-ttu-id="1cfdf-132">任何彙總練習期間是發揮最大可用的資源。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-132">During any consolidation exercise it is important to make maximum use of available resources.</span></span> <span data-ttu-id="1cfdf-133">如先前所討論，SQL 資料磁碟區上的儲存體 I/O 會在 BizTalk Server 解決方案的整體效能中扮演重要部分。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-133">As discussed previously, storage I/O on SQL data volumes plays a significant part in the overall performance of a BizTalk Server solution.</span></span> <span data-ttu-id="1cfdf-134">因此本指南中，已通過測試的實體磁碟的 HYPER-V 中的直接存取磁碟效能的相對效能。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-134">Therefore as part of this guidance, the relative performance of a physical disk to the performance of a passthrough disk in Hyper-V was tested.</span></span> <span data-ttu-id="1cfdf-135">Physical_SQL01 中的相對效能的 MessageBox 資料磁碟機和 Virtual_SQL01 測量使用的 IOMeter 開放原始碼工具原本由 Intel Corporation 發展和現在維護由開啟來源開發實驗室 (OSDL)。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-135">The relative performance of the MessageBox data drive in Physical_SQL01 and Virtual_SQL01 was measured using the IOMeter open source tool originally developed by Intel Corporation and now maintained by the open Source Development Lab (OSDL).</span></span> <span data-ttu-id="1cfdf-136">如需 IOMeter 的詳細資訊，請參閱[ http://go.microsoft.com/fwlink/?LinkId=122412 ](http://go.microsoft.com/fwlink/?LinkId=122412)。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-136">For more information about IOMeter, see [http://go.microsoft.com/fwlink/?LinkId=122412](http://go.microsoft.com/fwlink/?LinkId=122412).</span></span>  
  
 <span data-ttu-id="1cfdf-137">下表說明在測試環境、 所使用的 IOMeter 設定選項、 執行、 測試的描述和結果的摘要使用的實體和虛擬的硬體組態。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-137">The following tables describe the physical and virtual hardware configuration used in the test environment, the IOMeter configuration options that were used, a description of the test that was run, and a summary of results.</span></span>  
  
#### <a name="configuration-used-for-testing"></a><span data-ttu-id="1cfdf-138">用於測試的組態</span><span class="sxs-lookup"><span data-stu-id="1cfdf-138">Configuration Used for Testing</span></span>  
  
### <a name="physicalsql01"></a><span data-ttu-id="1cfdf-139">Physical_SQL01</span><span class="sxs-lookup"><span data-stu-id="1cfdf-139">Physical_SQL01</span></span>  
  
|||  
|-|-|  
|<span data-ttu-id="1cfdf-140">**模型**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-140">**Model**</span></span>|<span data-ttu-id="1cfdf-141">HP DL580</span><span class="sxs-lookup"><span data-stu-id="1cfdf-141">HP DL580</span></span>|  
|<span data-ttu-id="1cfdf-142">**Processor**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-142">**Processor**</span></span>|<span data-ttu-id="1cfdf-143">四處理器，顆四核心 Intel Xeon 2.4 Ghz</span><span class="sxs-lookup"><span data-stu-id="1cfdf-143">Quad processor, Quad-core Intel Xeon 2.4Ghz</span></span>|  
|<span data-ttu-id="1cfdf-144">**記憶體**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-144">**Memory**</span></span>|<span data-ttu-id="1cfdf-145">8 GB</span><span class="sxs-lookup"><span data-stu-id="1cfdf-145">8 GB</span></span>|  
|<span data-ttu-id="1cfdf-146">**網路功能**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-146">**Networking**</span></span>|<span data-ttu-id="1cfdf-147">HP NC3T3i 多功能十億位元伺服器的配接器</span><span class="sxs-lookup"><span data-stu-id="1cfdf-147">HP NC3T3i Multifunction Gigabit Server adapter</span></span>|  
|<span data-ttu-id="1cfdf-148">**SAN 組態**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-148">**SAN configuration**</span></span>|<span data-ttu-id="1cfdf-149">直接連接的 SAN 存放裝置 （請參閱下表）</span><span class="sxs-lookup"><span data-stu-id="1cfdf-149">Direct attached SAN storage (see table below)</span></span>|  
  
### <a name="physicalsql01--san-configuration"></a><span data-ttu-id="1cfdf-150">Physical_SQL01 – SAN 組態</span><span class="sxs-lookup"><span data-stu-id="1cfdf-150">Physical_SQL01 – SAN Configuration</span></span>  
  
|<span data-ttu-id="1cfdf-151">磁碟機代號</span><span class="sxs-lookup"><span data-stu-id="1cfdf-151">Drive letter</span></span>|<span data-ttu-id="1cfdf-152">Description</span><span class="sxs-lookup"><span data-stu-id="1cfdf-152">Description</span></span>|<span data-ttu-id="1cfdf-153">LUN 的大小</span><span class="sxs-lookup"><span data-stu-id="1cfdf-153">LUN Size</span></span>|<span data-ttu-id="1cfdf-154">RAID 組態</span><span class="sxs-lookup"><span data-stu-id="1cfdf-154">RAID configuration</span></span>|  
|------------------|-----------------|--------------|------------------------|  
|<span data-ttu-id="1cfdf-155">G:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-155">G:</span></span>|<span data-ttu-id="1cfdf-156">Data_Sys</span><span class="sxs-lookup"><span data-stu-id="1cfdf-156">Data_Sys</span></span>|<span data-ttu-id="1cfdf-157">10</span><span class="sxs-lookup"><span data-stu-id="1cfdf-157">10</span></span>|<span data-ttu-id="1cfdf-158">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-158">RAID 0 + 1</span></span>|  
|<span data-ttu-id="1cfdf-159">H:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-159">H:</span></span>|<span data-ttu-id="1cfdf-160">Logs_Sys</span><span class="sxs-lookup"><span data-stu-id="1cfdf-160">Logs_Sys</span></span>|<span data-ttu-id="1cfdf-161">10</span><span class="sxs-lookup"><span data-stu-id="1cfdf-161">10</span></span>|<span data-ttu-id="1cfdf-162">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-162">RAID 0 + 1</span></span>|  
|<span data-ttu-id="1cfdf-163">I:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-163">I:</span></span>|<span data-ttu-id="1cfdf-164">Data_TempDb</span><span class="sxs-lookup"><span data-stu-id="1cfdf-164">Data_TempDb</span></span>|<span data-ttu-id="1cfdf-165">50</span><span class="sxs-lookup"><span data-stu-id="1cfdf-165">50</span></span>|<span data-ttu-id="1cfdf-166">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-166">RAID 0 + 1</span></span>|  
|<span data-ttu-id="1cfdf-167">J:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-167">J:</span></span>|<span data-ttu-id="1cfdf-168">Logs_TempDb</span><span class="sxs-lookup"><span data-stu-id="1cfdf-168">Logs_TempDb</span></span>|<span data-ttu-id="1cfdf-169">50</span><span class="sxs-lookup"><span data-stu-id="1cfdf-169">50</span></span>|<span data-ttu-id="1cfdf-170">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-170">RAID 0 + 1</span></span>|  
|<span data-ttu-id="1cfdf-171">K:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-171">K:</span></span>|<span data-ttu-id="1cfdf-172">Data_BtsMsgBox</span><span class="sxs-lookup"><span data-stu-id="1cfdf-172">Data_BtsMsgBox</span></span>|<span data-ttu-id="1cfdf-173">300</span><span class="sxs-lookup"><span data-stu-id="1cfdf-173">300</span></span>|<span data-ttu-id="1cfdf-174">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-174">RAID 0 + 1</span></span>|  
|<span data-ttu-id="1cfdf-175">L:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-175">L:</span></span>|<span data-ttu-id="1cfdf-176">Logs_BtsMsgBox</span><span class="sxs-lookup"><span data-stu-id="1cfdf-176">Logs_BtsMsgBox</span></span>|<span data-ttu-id="1cfdf-177">100</span><span class="sxs-lookup"><span data-stu-id="1cfdf-177">100</span></span>|<span data-ttu-id="1cfdf-178">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-178">RAID 0 + 1</span></span>|  
|<span data-ttu-id="1cfdf-179">M:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-179">M:</span></span>|<span data-ttu-id="1cfdf-180">MSDTC</span><span class="sxs-lookup"><span data-stu-id="1cfdf-180">MSDTC</span></span>|<span data-ttu-id="1cfdf-181">5</span><span class="sxs-lookup"><span data-stu-id="1cfdf-181">5</span></span>|<span data-ttu-id="1cfdf-182">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-182">RAID 0 + 1</span></span>|  
  
### <a name="hyper-vhostsql01"></a><span data-ttu-id="1cfdf-183">Hyper-V_Host_SQL01</span><span class="sxs-lookup"><span data-stu-id="1cfdf-183">Hyper-V_Host_SQL01</span></span>  
  
|||  
|-|-|  
|<span data-ttu-id="1cfdf-184">**模型**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-184">**Model**</span></span>|<span data-ttu-id="1cfdf-185">HP DL580</span><span class="sxs-lookup"><span data-stu-id="1cfdf-185">HP DL580</span></span>|  
|<span data-ttu-id="1cfdf-186">**Processor**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-186">**Processor**</span></span>|<span data-ttu-id="1cfdf-187">四處理器，顆四核心 Intel Xeon 2.4 Ghz</span><span class="sxs-lookup"><span data-stu-id="1cfdf-187">Quad processor, Quad-core Intel Xeon 2.4Ghz</span></span>|  
|<span data-ttu-id="1cfdf-188">**記憶體**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-188">**Memory**</span></span>|<span data-ttu-id="1cfdf-189">32 GB</span><span class="sxs-lookup"><span data-stu-id="1cfdf-189">32 GB</span></span>|  
|<span data-ttu-id="1cfdf-190">**網路功能**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-190">**Networking**</span></span>|<span data-ttu-id="1cfdf-191">Broadcom BCM5708C NetXtreme II GigEHP DL380 G5</span><span class="sxs-lookup"><span data-stu-id="1cfdf-191">Broadcom BCM5708C NetXtreme II GigEHP DL380 G5</span></span>|  
  
### <a name="virtualsql01---virtual-machine-configuration"></a><span data-ttu-id="1cfdf-192">Virtual_SQL01-虛擬機器組態</span><span class="sxs-lookup"><span data-stu-id="1cfdf-192">Virtual_SQL01 - Virtual Machine Configuration</span></span>  
  
|||  
|-|-|  
|<span data-ttu-id="1cfdf-193">**虛擬處理器**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-193">**Virtual processors**</span></span>|<span data-ttu-id="1cfdf-194">配置 4</span><span class="sxs-lookup"><span data-stu-id="1cfdf-194">4 allocated</span></span>|  
|<span data-ttu-id="1cfdf-195">**記憶體**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-195">**Memory**</span></span>|<span data-ttu-id="1cfdf-196">8 GB</span><span class="sxs-lookup"><span data-stu-id="1cfdf-196">8 GB</span></span>|  
|<span data-ttu-id="1cfdf-197">**網路功能**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-197">**Networking**</span></span>|<span data-ttu-id="1cfdf-198">虛擬機器的網路連線到：</span><span class="sxs-lookup"><span data-stu-id="1cfdf-198">Virtual Machine Networking connected to:</span></span><br /><span data-ttu-id="1cfdf-199">Broadcom BCM5708C NetXtreme II GigE</span><span class="sxs-lookup"><span data-stu-id="1cfdf-199">Broadcom BCM5708C NetXtreme II GigE</span></span>|  
|<span data-ttu-id="1cfdf-200">**硬碟設定**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-200">**Hard disk configuration**</span></span>|<span data-ttu-id="1cfdf-201">**IDE 控制器**– 30 GB 的作業系統固定 vhd</span><span class="sxs-lookup"><span data-stu-id="1cfdf-201">**IDE controller** – 30 GB fixed vhd for Operating System</span></span><br /><span data-ttu-id="1cfdf-202">**SCSI 控制器**-7 直接連接直接存取 SAN Lun （請參閱下表）</span><span class="sxs-lookup"><span data-stu-id="1cfdf-202">**SCSI controller** - 7 directly attached passthrough SAN LUNs (see table below)</span></span>|  
  
### <a name="virtualsql01--san-configuration"></a><span data-ttu-id="1cfdf-203">Virtual_SQL01 – SAN Configuration</span><span class="sxs-lookup"><span data-stu-id="1cfdf-203">Virtual_SQL01 – SAN Configuration</span></span>  
  
|<span data-ttu-id="1cfdf-204">磁碟機代號</span><span class="sxs-lookup"><span data-stu-id="1cfdf-204">Drive letter</span></span>|<span data-ttu-id="1cfdf-205">Description</span><span class="sxs-lookup"><span data-stu-id="1cfdf-205">Description</span></span>|<span data-ttu-id="1cfdf-206">LUN 的大小</span><span class="sxs-lookup"><span data-stu-id="1cfdf-206">LUN Size</span></span>|<span data-ttu-id="1cfdf-207">RAID 組態</span><span class="sxs-lookup"><span data-stu-id="1cfdf-207">RAID configuration</span></span>|  
|------------------|-----------------|--------------|------------------------|  
|<span data-ttu-id="1cfdf-208">G:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-208">G:</span></span>|<span data-ttu-id="1cfdf-209">Data_Sys</span><span class="sxs-lookup"><span data-stu-id="1cfdf-209">Data_Sys</span></span>|<span data-ttu-id="1cfdf-210">10</span><span class="sxs-lookup"><span data-stu-id="1cfdf-210">10</span></span>|<span data-ttu-id="1cfdf-211">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-211">RAID 0 + 1</span></span>|  
|<span data-ttu-id="1cfdf-212">H:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-212">H:</span></span>|<span data-ttu-id="1cfdf-213">Logs_Sys</span><span class="sxs-lookup"><span data-stu-id="1cfdf-213">Logs_Sys</span></span>|<span data-ttu-id="1cfdf-214">10</span><span class="sxs-lookup"><span data-stu-id="1cfdf-214">10</span></span>|<span data-ttu-id="1cfdf-215">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-215">RAID 0 + 1</span></span>|  
|<span data-ttu-id="1cfdf-216">I:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-216">I:</span></span>|<span data-ttu-id="1cfdf-217">Data_TempDb</span><span class="sxs-lookup"><span data-stu-id="1cfdf-217">Data_TempDb</span></span>|<span data-ttu-id="1cfdf-218">50</span><span class="sxs-lookup"><span data-stu-id="1cfdf-218">50</span></span>|<span data-ttu-id="1cfdf-219">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-219">RAID 0 + 1</span></span>|  
|<span data-ttu-id="1cfdf-220">J:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-220">J:</span></span>|<span data-ttu-id="1cfdf-221">Logs_TempDb</span><span class="sxs-lookup"><span data-stu-id="1cfdf-221">Logs_TempDb</span></span>|<span data-ttu-id="1cfdf-222">50</span><span class="sxs-lookup"><span data-stu-id="1cfdf-222">50</span></span>|<span data-ttu-id="1cfdf-223">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-223">RAID 0 + 1</span></span>|  
|<span data-ttu-id="1cfdf-224">K:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-224">K:</span></span>|<span data-ttu-id="1cfdf-225">Data_BtsMsgBox</span><span class="sxs-lookup"><span data-stu-id="1cfdf-225">Data_BtsMsgBox</span></span>|<span data-ttu-id="1cfdf-226">300</span><span class="sxs-lookup"><span data-stu-id="1cfdf-226">300</span></span>|<span data-ttu-id="1cfdf-227">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-227">RAID 0 + 1</span></span>|  
|<span data-ttu-id="1cfdf-228">L:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-228">L:</span></span>|<span data-ttu-id="1cfdf-229">Logs_BtsMsgBox</span><span class="sxs-lookup"><span data-stu-id="1cfdf-229">Logs_BtsMsgBox</span></span>|<span data-ttu-id="1cfdf-230">100</span><span class="sxs-lookup"><span data-stu-id="1cfdf-230">100</span></span>|<span data-ttu-id="1cfdf-231">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-231">RAID 0 + 1</span></span>|  
|<span data-ttu-id="1cfdf-232">M:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-232">M:</span></span>|<span data-ttu-id="1cfdf-233">MSDTC</span><span class="sxs-lookup"><span data-stu-id="1cfdf-233">MSDTC</span></span>|<span data-ttu-id="1cfdf-234">5</span><span class="sxs-lookup"><span data-stu-id="1cfdf-234">5</span></span>|<span data-ttu-id="1cfdf-235">RAID 0 + 1</span><span class="sxs-lookup"><span data-stu-id="1cfdf-235">RAID 0 + 1</span></span>|  
  
#### <a name="iometer-configuration"></a><span data-ttu-id="1cfdf-236">IOMeter 設定</span><span class="sxs-lookup"><span data-stu-id="1cfdf-236">IOMeter Configuration</span></span>  
 <span data-ttu-id="1cfdf-237">IOMeter 工具可用來當作基準測試和疑難排解工具藉由複寫應用程式的讀取/寫入效能。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-237">The IOMeter tool can be used as a benchmark and troubleshooting tool by replicating the read/write performance of applications.</span></span> <span data-ttu-id="1cfdf-238">IOMeter 是效能的一種可設定的工具，可用來模擬許多不同類型。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-238">IOMeter is a configurable tool that can be used to simulate many different types of performance.</span></span> <span data-ttu-id="1cfdf-239">基於此測試案例的詳細資訊，IOMeter 組態參數已設定為下表中所述的兩個實體[!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)]已測試的電腦和客體作業系統執行[!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)]在 HYPER-V 虛擬機器:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-239">For purposes of this test scenario, IOMeter configuration parameters were set as described in the table below on both the physical [!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)] computer that was tested and on the guest operating system that was running [!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)] in a Hyper-V virtual machine:</span></span>  
  
### <a name="iometer--passthrough-disk-comparison-test-configuration"></a><span data-ttu-id="1cfdf-240">IOMeter – 穿通磁碟比較測試組態</span><span class="sxs-lookup"><span data-stu-id="1cfdf-240">IOMeter – Passthrough Disk Comparison Test Configuration</span></span>  
  
|||  
|-|-|  
|<span data-ttu-id="1cfdf-241">**測試時間長度**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-241">**Test length**</span></span>|<span data-ttu-id="1cfdf-242">10 分鐘</span><span class="sxs-lookup"><span data-stu-id="1cfdf-242">10 minutes</span></span>|  
|<span data-ttu-id="1cfdf-243">**準備時間**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-243">**Ramp up time**</span></span>|<span data-ttu-id="1cfdf-244">30 秒</span><span class="sxs-lookup"><span data-stu-id="1cfdf-244">30 seconds</span></span>|  
|<span data-ttu-id="1cfdf-245">**背景工作數目**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-245">**Number of workers**</span></span>|<span data-ttu-id="1cfdf-246">4</span><span class="sxs-lookup"><span data-stu-id="1cfdf-246">4</span></span>|  
|<span data-ttu-id="1cfdf-247">**傳送要求的大小**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-247">**Transfer request size**</span></span>|<span data-ttu-id="1cfdf-248">2 KB</span><span class="sxs-lookup"><span data-stu-id="1cfdf-248">2 KB</span></span>|  
|<span data-ttu-id="1cfdf-249">**讀取/寫入發佈**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-249">**Read/write distribution**</span></span>|<span data-ttu-id="1cfdf-250">讀取 66%，33%寫入</span><span class="sxs-lookup"><span data-stu-id="1cfdf-250">66% read, 33% write</span></span>|  
|<span data-ttu-id="1cfdf-251">**高載長度**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-251">**Burst length**</span></span>|<span data-ttu-id="1cfdf-252">1 的 i/o 數目</span><span class="sxs-lookup"><span data-stu-id="1cfdf-252">1 I/Os</span></span>|  
|<span data-ttu-id="1cfdf-253">**目標磁碟機**</span><span class="sxs-lookup"><span data-stu-id="1cfdf-253">**Target Drive**</span></span>|<span data-ttu-id="1cfdf-254">K:\\</span><span class="sxs-lookup"><span data-stu-id="1cfdf-254">K:\\</span></span>|  
  
#### <a name="test-description"></a><span data-ttu-id="1cfdf-255">測試描述</span><span class="sxs-lookup"><span data-stu-id="1cfdf-255">Test Description</span></span>  
 <span data-ttu-id="1cfdf-256">[!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)]服務已停止，確保 IOMeter 已執行對磁碟 I/O 的唯一程序的兩部伺服器上。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-256">The [!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)] service was stopped on both servers to ensure that IOMeter was the only process performing I/O against the disk.</span></span> <span data-ttu-id="1cfdf-257">LUN 的使用在這項測試都位於相同的 SAN 專用於此實驗室環境。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-257">The LUN’s used in this test were both located on the same SAN which was dedicated to this lab environment.</span></span> <span data-ttu-id="1cfdf-258">任何其他的 I/O 活動時不執行測試，確保結果不有所偏差針對 SAN。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-258">No other I/O activity was performed against the SAN during the test to ensure that the results were not skewed.</span></span> <span data-ttu-id="1cfdf-259">藉由從每個在本機執行 IOMeter 工具再執行測試[!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)]和收集下列效能監視器計數器：</span><span class="sxs-lookup"><span data-stu-id="1cfdf-259">The test was then run by executing the IOMeter tool locally from each [!INCLUDE[btsSQLServerNoVersion](../includes/btssqlservernoversion-md.md)] and the following performance monitor counters were collected:</span></span>  
  
 <span data-ttu-id="1cfdf-260">**收集 Virtual_SQL01 和 Physical_SQL01**:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-260">**Collected from both Virtual_SQL01 and Physical_SQL01**:</span></span>  
  
-   <span data-ttu-id="1cfdf-261">\LogicalDisk(*)\\\*</span><span class="sxs-lookup"><span data-stu-id="1cfdf-261">\LogicalDisk(*)\\\*</span></span>  
  
-   <span data-ttu-id="1cfdf-262">\PhysicalDisk(*)\\\*</span><span class="sxs-lookup"><span data-stu-id="1cfdf-262">\PhysicalDisk(*)\\\*</span></span>  
  
 <span data-ttu-id="1cfdf-263">**從虛擬機器 Hyper-v V_02 收集**:</span><span class="sxs-lookup"><span data-stu-id="1cfdf-263">**Collected from virtual machine Hyper-V_02**:</span></span>  
  
-   <span data-ttu-id="1cfdf-264">\Hyper-V 虛擬存放裝置\\*</span><span class="sxs-lookup"><span data-stu-id="1cfdf-264">\Hyper-V Virtual Storage Device\\*</span></span>  
  
### <a name="results"></a><span data-ttu-id="1cfdf-265">結果</span><span class="sxs-lookup"><span data-stu-id="1cfdf-265">Results</span></span>  
 <span data-ttu-id="1cfdf-266">傳遞磁碟已達到超過 90%的直接連線到 Physical_SQL01 的 SAN LUN 的輸送量。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-266">The passthrough disk was able to attain over 90% of the throughput of the SAN LUN connected directly to Physical_SQL01.</span></span>  <span data-ttu-id="1cfdf-267">總計、 讀取和寫入每秒的 I/o 已全部都在 10%內為每秒傳送的總 MB。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-267">Total, read and write I/Os per second were all within 10% as was the total MB transferred per second.</span></span>  <span data-ttu-id="1cfdf-268">狀況良好的磁碟回應時間應該介於 1-15 毫秒的讀取和寫入。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-268">Response times for healthy disks should be between 1-15 ms for read and write.</span></span> <span data-ttu-id="1cfdf-269">I/O 的平均回應時間的兩個磁碟小於 4 毫秒。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-269">Average I/O response times were less than 4 ms on both disks.</span></span> <span data-ttu-id="1cfdf-270">隨機讀取回應時間為 5.4 ms 實體上和 5.7 ms 傳遞磁碟上。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-270">Random reads response time was 5.4 ms on the physical and 5.7 ms on the pass-through disk.</span></span> <span data-ttu-id="1cfdf-271">寫入回應時間為小於 0.5 毫秒在實體和虛擬環境。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-271">Write response time was less than 0.5 ms on both the physical and virtual environments.</span></span>  
  
 <span data-ttu-id="1cfdf-272">結果會指出，使用已啟用功能的 SCSI 控制器的直接存取磁碟可以提供超過 90%的直接連接的實體磁碟的效能。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-272">The results indicate that a passthrough disk using the enlightened SCSI controller can provide over 90% of the performance of a directly connected physical disk.</span></span> <span data-ttu-id="1cfdf-273">I/O 子系統效能非常重要，有效的 BizTalk Server 作業，藉由提供極佳的輸送量和回應時間 HYPER-V 是傑出候選項目合併 BizTalk Server 環境。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-273">I/O subsystem performance is critical for efficient BizTalk Server operation, by providing excellent throughput and response times Hyper-V is an excellent candidate for consolidating a BizTalk Server environment.</span></span> <span data-ttu-id="1cfdf-274">下表提供比較效能的實體磁碟將穿通磁碟時，觀察到的磁碟測試結果摘要：</span><span class="sxs-lookup"><span data-stu-id="1cfdf-274">The table below provides a summary of the disk test results observed when comparing performance of a passthrough disk to a physical disk:</span></span>  
  
|<span data-ttu-id="1cfdf-275">度量</span><span class="sxs-lookup"><span data-stu-id="1cfdf-275">Measurement</span></span>|<span data-ttu-id="1cfdf-276">Physical_SQL01 （實體磁碟）</span><span class="sxs-lookup"><span data-stu-id="1cfdf-276">Physical_SQL01 (Physical Disk)</span></span>|<span data-ttu-id="1cfdf-277">Virtual_SQL01 (passthrough)</span><span class="sxs-lookup"><span data-stu-id="1cfdf-277">Virtual_SQL01 (passthrough)</span></span>|<span data-ttu-id="1cfdf-278">傳遞磁碟的實體磁碟的相對效能</span><span class="sxs-lookup"><span data-stu-id="1cfdf-278">Relative performance of passthrough disks to physical disks</span></span>|  
|-----------------|---------------------------------------|------------------------------------|-----------------------------------------------------------------|  
|<span data-ttu-id="1cfdf-279">每秒總 I/o</span><span class="sxs-lookup"><span data-stu-id="1cfdf-279">Total I/Os per second</span></span>|<span data-ttu-id="1cfdf-280">269.73</span><span class="sxs-lookup"><span data-stu-id="1cfdf-280">269.73</span></span>|<span data-ttu-id="1cfdf-281">250.47</span><span class="sxs-lookup"><span data-stu-id="1cfdf-281">250.47</span></span>|<span data-ttu-id="1cfdf-282">92.86%</span><span class="sxs-lookup"><span data-stu-id="1cfdf-282">92.86%</span></span>|  
|<span data-ttu-id="1cfdf-283">每秒的讀取 I/o</span><span class="sxs-lookup"><span data-stu-id="1cfdf-283">Read I/Os per second</span></span>|<span data-ttu-id="1cfdf-284">180.73</span><span class="sxs-lookup"><span data-stu-id="1cfdf-284">180.73</span></span>|<span data-ttu-id="1cfdf-285">167.60</span><span class="sxs-lookup"><span data-stu-id="1cfdf-285">167.60</span></span>|<span data-ttu-id="1cfdf-286">92.74%</span><span class="sxs-lookup"><span data-stu-id="1cfdf-286">92.74%</span></span>|  
|<span data-ttu-id="1cfdf-287">每秒寫入 I/o</span><span class="sxs-lookup"><span data-stu-id="1cfdf-287">Write I/Os per second</span></span>|<span data-ttu-id="1cfdf-288">89.00</span><span class="sxs-lookup"><span data-stu-id="1cfdf-288">89.00</span></span>|<span data-ttu-id="1cfdf-289">82.87</span><span class="sxs-lookup"><span data-stu-id="1cfdf-289">82.87</span></span>|<span data-ttu-id="1cfdf-290">93.11%</span><span class="sxs-lookup"><span data-stu-id="1cfdf-290">93.11%</span></span>|  
|<span data-ttu-id="1cfdf-291">每秒的總 Mb</span><span class="sxs-lookup"><span data-stu-id="1cfdf-291">Total MBs per second</span></span>|<span data-ttu-id="1cfdf-292">0.53</span><span class="sxs-lookup"><span data-stu-id="1cfdf-292">0.53</span></span>|<span data-ttu-id="1cfdf-293">0.49</span><span class="sxs-lookup"><span data-stu-id="1cfdf-293">0.49</span></span>|<span data-ttu-id="1cfdf-294">92.45%</span><span class="sxs-lookup"><span data-stu-id="1cfdf-294">92.45%</span></span>|  
|<span data-ttu-id="1cfdf-295">平均讀取回應時間 （毫秒）</span><span class="sxs-lookup"><span data-stu-id="1cfdf-295">Average read response time (ms)</span></span>|<span data-ttu-id="1cfdf-296">5.4066</span><span class="sxs-lookup"><span data-stu-id="1cfdf-296">5.4066</span></span>|<span data-ttu-id="1cfdf-297">5.7797</span><span class="sxs-lookup"><span data-stu-id="1cfdf-297">5.7797</span></span>|<span data-ttu-id="1cfdf-298">93.54%</span><span class="sxs-lookup"><span data-stu-id="1cfdf-298">93.54%</span></span>|  
|<span data-ttu-id="1cfdf-299">寫入的平均回應時間 （毫秒）</span><span class="sxs-lookup"><span data-stu-id="1cfdf-299">Average write response time (ms)</span></span>|<span data-ttu-id="1cfdf-300">0.2544</span><span class="sxs-lookup"><span data-stu-id="1cfdf-300">0.2544</span></span>|<span data-ttu-id="1cfdf-301">0.3716</span><span class="sxs-lookup"><span data-stu-id="1cfdf-301">0.3716</span></span>|<span data-ttu-id="1cfdf-302">68.42%**附註：** 雖然傳遞磁碟寫入的平均回應時間的相對效能 68.42%的實體磁碟的效能，以及內已傳遞磁碟的平均寫入回應時間建立可接受的限制為 10 毫秒。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-302">68.42% **Note:**  Although the relative performance of the pass through disks for Average write response time was 68.42% of the performance of physical disks, the Average write response time of the passthrough disks was still well within established acceptable limits of 10 ms.</span></span>|  
|<span data-ttu-id="1cfdf-303">平均 I/O 回應時間 （毫秒）</span><span class="sxs-lookup"><span data-stu-id="1cfdf-303">Average I/O response time (ms)</span></span>|<span data-ttu-id="1cfdf-304">3.7066</span><span class="sxs-lookup"><span data-stu-id="1cfdf-304">3.7066</span></span>|<span data-ttu-id="1cfdf-305">3.9904</span><span class="sxs-lookup"><span data-stu-id="1cfdf-305">3.9904</span></span>|<span data-ttu-id="1cfdf-306">93.89%</span><span class="sxs-lookup"><span data-stu-id="1cfdf-306">93.89%</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="1cfdf-307">每秒總 I/o、 每秒的讀取 I/o、 每秒寫入的 I/o 和每秒的總 mb/s 的百分比值會計算除以穿通磁碟值對應的實體磁碟值。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-307">The percentage values for Total I/Os per second, Read I/Os per second, Write I/Os per second, and Total MBs per second were calculated by dividing passthrough disk values by the corresponding physical disk values.</span></span>  
>   
>  <span data-ttu-id="1cfdf-308">百分比值的平均讀取回應時間 （毫秒），平均寫入回應時間 （毫秒），並計算實體磁碟值對應的傳遞磁碟值再除以出來的平均 I/O 回應時間 （毫秒）。</span><span class="sxs-lookup"><span data-stu-id="1cfdf-308">The percentage values for Average read response time (ms), Average write response time (ms), and Average I/O response time (ms) were calculated by dividing physical disk values by the corresponding passthrough disk values.</span></span>